{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 10.5k/10.5k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 733k/733k [00:00<00:00, 1.85MB/s]\n",
      "Downloading data: 100%|██████████| 6.36M/6.36M [00:00<00:00, 6.62MB/s]\n",
      "Downloading data: 100%|██████████| 657k/657k [00:00<00:00, 2.07MB/s]\n",
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 398273.82 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 1034447.36 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 589620.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_yn = pd.read_csv(\"C:/Users/USER/삼성화재/KU_ML/Dataset/고객별 사고 유무 기본 데이터.csv\", encoding = \"UTF-8\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yn['processed'] = df_yn.apply(lambda row: '/'.join(row.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.0/1.0/nan/B/신차/기타/가족및형제자매한정/8.0/5천만원이하/가입/15...\n",
       "1         0.0/1.0/nan/N/신차/기타/기명피보험자1인한정/8.0/미가입/미가입/150...\n",
       "2         0.0/1.0/nan/N/신차/기타/가족한정(형제자매제외)/5.0/5천만원이하/가입...\n",
       "3         0.0/1.0/1.0/N/10년이상/중형/가족한정(형제자매제외)/2.0/5천만원이하...\n",
       "4         0.0/1.0/1.0/Z/5년이하/다목적2종/가족한정(형제자매제외)/1.0/미가입/...\n",
       "                                ...                        \n",
       "267772    40.0/2.0/1.0/C/10년이상/중형/누구나(기본)/8.0/미가입/미가입/70...\n",
       "267773    40.0/2.0/1.0/C/5년이하/소형A/ 부부 및 자녀한정/8.0/5천만원이하/...\n",
       "267774    40.0/2.0/1.0/C/10년이하/소형A/기명피보험자1인한정/8.0/미가입/미가...\n",
       "267775    40.0/2.0/1.0/C/5년이하/소형B/누구나(기본)/7.0/5천만원이하/가입/...\n",
       "267776    40.0/2.0/1.0/C/10년이상/소형A/기명피보험자1인한정/6.0/미가입/미가...\n",
       "Name: processed, Length: 267777, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yn[\"processed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_yn['processed'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformersNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "     ---------------------------------------- 0.0/138.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 138.0/138.0 kB 8.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\program files\\python310\\lib\\site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\program files\\python310\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\program files\\python310\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.4.28-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.9/41.9 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\program files\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\program files\\python310\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\program files\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\program files\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python310\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python310\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python310\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/9.0 MB 11.2 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.8/9.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.3/9.0 MB 10.0 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.7/9.0 MB 10.0 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.2/9.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.7/9.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.1/9.0 MB 10.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.6/9.0 MB 10.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.6/9.0 MB 10.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.1/9.0 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.4/9.0 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.9/9.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.5/9.0 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.9/9.0 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.4/9.0 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.9/9.0 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.3/9.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.8/9.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.1/9.0 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.5/9.0 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.9/9.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.0/9.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.0/9.0 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading regex-2024.4.28-cp310-cp310-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/269.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 269.0/269.0 kB 8.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp310-none-win_amd64.whl (287 kB)\n",
      "   ---------------------------------------- 0.0/287.4 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 245.8/287.4 kB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 287.4/287.4 kB 5.9 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.4/2.2 MB 12.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.2 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.4/2.2 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.4/2.2 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.4/2.2 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 6.4 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.4.28 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.40.1\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\program files\\python310\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\program files\\python310\\lib\\site-packages (from tokenizers) (0.22.2)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\program files\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\program files\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\program files\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\program files\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\program files\\python310\\lib\\site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python310\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python310\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python310\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python310\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 리스트를 텍스트 파일로 저장\n",
    "with open(\"sentences.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for sentence in sentences:\n",
    "        file.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 WordPiece 토크나이저 초기화\n",
    "tokenizer = BertWordPieceTokenizer(clean_text=True, handle_chinese_chars=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\bert-wordpiece-vocab.txt']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저 훈련\n",
    "tokenizer.train(\n",
    "    files=[\"sentences.txt\"],\n",
    "    vocab_size=30522,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    limit_alphabet=1000,\n",
    "    wordpieces_prefix=\"##\"\n",
    ")\n",
    "\n",
    "# 훈련된 토크나이저 저장\n",
    "tokenizer.save_model(\".\", \"bert-wordpiece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error while initializing WordPiece: 지정된 경로를 찾을 수 없습니다. (os error 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mUntitled-2.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-2.ipynb?jupyter-notebook#X23sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtokenizers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertWordPieceTokenizer\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-2.ipynb?jupyter-notebook#X23sdW50aXRsZWQ%3D?line=2'>3</a>\u001b[0m \u001b[39m# 저장된 토크나이저 모델 로드\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-2.ipynb?jupyter-notebook#X23sdW50aXRsZWQ%3D?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertWordPieceTokenizer(\u001b[39m\"\u001b[39;49m\u001b[39mbert-wordpiece/vocab.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, lowercase\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-2.ipynb?jupyter-notebook#X23sdW50aXRsZWQ%3D?line=5'>6</a>\u001b[0m \u001b[39m# 토크나이저로 문장을 토큰화하고 출력\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-2.ipynb?jupyter-notebook#X23sdW50aXRsZWQ%3D?line=6'>7</a>\u001b[0m test_sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m여기에 테스트할 문장을 입력하세요.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\tokenizers\\implementations\\bert_wordpiece.py:30\u001b[0m, in \u001b[0;36mBertWordPieceTokenizer.__init__\u001b[1;34m(self, vocab, unk_token, sep_token, cls_token, pad_token, mask_token, clean_text, handle_chinese_chars, strip_accents, lowercase, wordpieces_prefix)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     16\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     17\u001b[0m     vocab: Optional[Union[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     wordpieces_prefix: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m##\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     28\u001b[0m ):\n\u001b[0;32m     29\u001b[0m     \u001b[39mif\u001b[39;00m vocab \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m         tokenizer \u001b[39m=\u001b[39m Tokenizer(WordPiece(vocab, unk_token\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m(unk_token)))\n\u001b[0;32m     31\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m         tokenizer \u001b[39m=\u001b[39m Tokenizer(WordPiece(unk_token\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(unk_token)))\n",
      "\u001b[1;31mException\u001b[0m: Error while initializing WordPiece: 지정된 경로를 찾을 수 없습니다. (os error 3)"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# 저장된 토크나이저 모델 로드\n",
    "tokenizer = BertWordPieceTokenizer(\"bert-wordpiece/vocab.txt\", lowercase=True)\n",
    "\n",
    "# 토크나이저로 문장을 토큰화하고 출력\n",
    "test_sentence = \"여기에 테스트할 문장을 입력하세요.\"\n",
    "output = tokenizer.encode(test_sentence)\n",
    "print(output.tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
